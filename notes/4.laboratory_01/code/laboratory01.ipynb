{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the SparkSession object is created and initializated. This has the same effect as going to the console and type *pyspark*. Just that here is a Python object that is created that we can start to utilice for our programs. You can see that the response of the object has the version of the instaled *spark*, where is the master located, the app that is currently running and a link to the UI for more information of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lab01</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x202cce3b4d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('Lab01')\\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get the *spark* object, we can start to use all of the commands, objects and functions of the system. In this case we need to get the data from the folder flight-data, and create a DataFrame with that data. For that we use the function of spark **read**, that allows us to read some type of data (csv, txt, parquet, etc.), next we can give some options to the **read** function, in this specific problem, we give 2 configuration. \n",
    "\n",
    "* The first one *inferSchema*, is an instruction to the **read** function that it need to infer the data type of each column from the same data that is reading.\n",
    "* The second one *header*, is an instruction indicating that the data, in the first row, has the name for each of the columns.\n",
    "\n",
    "Then the last intruction to the **read** function is to indicate what type of file we are reading and in what route is located that file, for this case that we need to read a *csv* we use that function and indicate the path of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    "    .read\\\n",
    "    .option('inferSchema', 'True')\\\n",
    "    .option('header', 'True')\\\n",
    "    .csv(r'../../../data/flight-data/csv/2015-summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Dataframe created we now have the first intruction of the plan for the DataFrame. If you remember what you have read, *Spark* is lazy and doesnt excecutes any of the transformation or processes until is necessary. The excecution of the pipeline created by *Spark* is when: \n",
    "\n",
    "* Is necessary to view data in the console.\n",
    "* Is necessary to collect data to native objects in the respective language.\n",
    "* Is necessary to write to output data sources.\n",
    "\n",
    "So, if we get the *Physical Plan* from the Dataframe object, we will get just the FileScan (This is just the name of the read function in the *Physical Plan*) for the flight-data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [DEST_COUNTRY_NAME#227,ORIGIN_COUNTRY_NAME#228,count#229] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/jorge/OneDrive/Mis Documentos/Projects/Spark-The-Defini..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the function **take** to view the data in the console. This will be an action and activates the pipeline created by *Spark* for the Dataframe and it will show us the response in the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another function that doesn't get us a response is the function **sort**, if we see the response in the console if not data, but an object and if we see the *Physical Plan* of the DataFrame we'll see that anothe 2 steps were added to the Plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#229 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#229 ASC NULLS FIRST, 15000), ENSURE_REQUIREMENTS, [plan_id=619]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#227,ORIGIN_COUNTRY_NAME#228,count#229] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/jorge/OneDrive/Mis Documentos/Projects/Spark-The-Defini..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the function **sort** that is a wide transformation, by default the function will create 200 partitions of the data, we can change that configuration by changing the value in the configuration variable *spark.sql.shuffle.partitions* to the number of partitions that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"15000\")\n",
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of *Spark* is that we dont need to only excecute programs in Python, we can also use the SQL language to create *Physical Plans* for our transformations. For this is necesary to create the table in the Data Base motor, or in this case we'll create a *Temporary View* that allows us to access the data with SQL while the Spark Session is turn on, in the moment that we stop the Spark Session, the data in the *Temporary View* will desappear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView('flight_data_2015')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll get the number of times a country is reapeated in the Data, doing it with *SQL* and *Python* to see if the *Physical Plans* of the two tranformations paths are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DEST_COUNTRY_NAME,\n",
    "        COUNT(1)\n",
    "    FROM flight_data_2015\n",
    "    GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "\n",
    "dataFrameWay = flightData2015\\\n",
    "    .groupBy('DEST_COUNTRY_NAME')\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we see the response of the *explain* to each of the DataFrames we'll see that the *Physical Plans* of each one of them is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#227], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#227, 15000), ENSURE_REQUIREMENTS, [plan_id=641]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#227], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#227] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/jorge/OneDrive/Mis Documentos/Projects/Spark-The-Defini..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#227], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#227, 15000), ENSURE_REQUIREMENTS, [plan_id=654]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#227], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#227] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/jorge/OneDrive/Mis Documentos/Projects/Spark-The-Defini..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how does *Spark* work internally, we can start getting some interesting information from the data that we have. In this case we need to know what is the maximum number of flights to and from any location. For this we'll need to use the function **max**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this only gives us the maximum number of all the data, if we need to get the maximum of each one of the countries, we'll need the variable *DEST_COUNTRY_NAME* to used it as a varible for agruppation. When we use the function **groupBy** in SQL or Python, what we are doing is to apply the specifyc function to each a every value that the variable to group by has. In this case we'll group by the variable *DEST_COUNTRY_NAME* to the the **sum** of the number of flights there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DEST_COUNTRY_NAME, \n",
    "        sum(count) as destination_total\n",
    "    FROM flight_data_2015\n",
    "    GROUP BY DEST_COUNTRY_NAME\n",
    "    ORDER BY sum(count) DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if the see the *Physical Plan* we'll see that:\n",
    "\n",
    "1. It scans the data from the CSV.\n",
    "2. Then agregates the data in a partial sum of the data in the actual partitions.\n",
    "3. Then reorganice the data in the partitions for each agrupation\n",
    "4. Then get the final sum for each partition\n",
    "5. And finally it sorts the information from higher to lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#417L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#227,destination_total#417L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#227], functions=[sum(count#229)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#227, 15000), ENSURE_REQUIREMENTS, [plan_id=824]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#227], functions=[partial_sum(count#229)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#227,count#229] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/jorge/OneDrive/Mis Documentos/Projects/Spark-The-Defini..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
